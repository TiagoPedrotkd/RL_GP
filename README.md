# ðŸ¤– Reinforcement Learning Project

This repository contains the implementation of a Reinforcement Learning (RL) project developed as part of the RL course. The goal is to train and compare multiple RL agents on two distinct environments from the [Gymnasium](https://gymnasium.farama.org/) library, focusing on performance, convergence, and the trade-off between exploration and exploitation.

---

## ðŸ“Œ Objectives

- Solve **two different Gymnasium environments** using different RL algorithms.
- Compare the performance of algorithms such as Q-Learning, SARSA, Monte Carlo, and PPO.
- Visualize learning progress with reward curves, success rates, and convergence plots.
- Analyze agent behavior and interpret learned policies.

---

## ðŸ§± Project Structure

```
RL_GP/
|â”€â”€ notebooks/
|â”€â”€ src/
â”‚   â””â”€â”€ agents/             # RL algorithm implementations (Q-Learning, SARSA, Monte Carlo, etc.)
â”‚   â””â”€â”€ environments/       # Custom wrappers and environment configurations (Blackjack, Pendulum)
â”‚   â””â”€â”€ training/           # Training scripts and grid search utilities
â”‚   â””â”€â”€ evaluation/         # Evaluation scripts and comparative analysis
â”‚   â””â”€â”€ main.py             # Main entry point to run all experiments
â”œâ”€â”€ config/                 # YAML configuration files for experiments
â”œâ”€â”€ output/                 # Generated plots, HTML reports, and metrics for each experiment
â”‚   â””â”€â”€ analysis/       
â”‚       â”œâ”€â”€ montecarlo/
â”‚       â”œâ”€â”€ sarsa/
â”‚       â”œâ”€â”€ qlearning/
â”‚       â”œâ”€â”€ pendulum_qlearning/
â”‚       â””â”€â”€ pendulum_sarsa/
|   â””â”€â”€ checkpoints/
|   â””â”€â”€ best_qtable.npy
â”œâ”€â”€ logs/                   # Training logs for each experiment
â”œâ”€â”€ report/                 # Final HTML report and supporting files
â”œâ”€â”€ LICENSE
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md               # This file

```

---

## âš™ï¸ Installation

1. Clone this repository:

```bash
git clone https://github.com/TiagoPedrotkd/RL_GP.git
cd RL_GP
```

2. *(Optional)* Create and activate a virtual environment:

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install required packages:

```bash
pip install -r requirements.txt
```

---

## ðŸš€ How to Run

### Run All Experiments (Blackjack and Pendulum)

```bash
python -m src.main
```

This will:
- Train all agents (Monte Carlo, SARSA, Q-Learning, Random) on Blackjack.
- Train Q-Learning and SARSA agents on Pendulum.
- Perform grid search for each agent/policy.
- Save all logs, plots, and HTML reports in `output/analysis/` and `logs/`.

### Run Only Blackjack Experiments

```bash
python -m src.training.train_blackjack
```

### Run Only Pendulum Experiments

```bash
python -m src.training.train_pendulum
```

> All training metrics, visualizations, and HTML reports will be saved in the `output/analysis/` directory, organized by agent and policy.

---

## ðŸ“Š Visualizations & Reports

- **Learning curves** (moving average of returns)
- **Return histograms**
- **Q-table heatmaps** and **policy visualizations**
- **Comparative plots** across policies and agents
- **HTML reports** for each experiment and a global comparative report in `report/report.html`

---

## ðŸ“˜ Report

A detailed description of the methodology, evaluation metrics, challenges, and results is available in [`report/report.html`](./report/report.html) and [`report.pdf`](./report.pdf).

---

## ðŸ‘¥ Authors

Developed by:

- Tiago Pedro (tiagopedrosoares02@gmail.com)
- TomÃ¡s Silva (tomasestrociosilva@gmail.com)
- Cadmo Diogo (cadmaya@hotmail.com)

---

## ðŸ“„ License

This project is for educational purposes only. All rights reserved by the authors.
